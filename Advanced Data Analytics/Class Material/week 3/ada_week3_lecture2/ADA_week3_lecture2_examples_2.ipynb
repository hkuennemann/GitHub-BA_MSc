{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e90bac",
   "metadata": {
    "id": "d9e90bac"
   },
   "source": [
    "# Advanced Data Analysis - week 6, more examples\n",
    "\n",
    "In the advanced data analysis course, we assume basic knowledge of Python, as could be acquired by attending the Introduction to Programming bridging course.\n",
    "\n",
    "This notebook includes the examples and exercises presented in Week 6. There is an additional notebook with the examples and exercises suggested for autonomous study during the week.\n",
    "\n",
    "In week 6, we will focus on introducing Spark.\n",
    "\n",
    "\n",
    "[//]: # (We will be using latex for fomulas)\n",
    "\n",
    "<script type=\"text/javascript\"\n",
    "        src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML\"></script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf4023",
   "metadata": {},
   "source": [
    "# Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1b2ec2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe1b2ec2",
    "outputId": "8cdcecb3-8f83-4ed3-ed48-8eae2a6760c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: apt-get: command not found\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/310.8 MB\u001b[0m \u001b[31m611.6 kB/s\u001b[0m eta \u001b[36m0:07:54\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/http/client.py\", line 466, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/ssl.py\", line 1278, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/ssl.py\", line 1134, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/req_command.py\", line 248, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "                            ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 397, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
      "    return bool(self._sequence)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "           ^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "                ^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "                                       ^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 293, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 538, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 609, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "                 ^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\n",
      "    file = get_http_url(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 107, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/network/download.py\", line 147, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/contextlib.py\", line 155, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/Users/hendrik/anaconda3/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: filelock in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/hendrik/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-4.7.1\n",
      "/Users/hendrik/anaconda3/lib/python3.11/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Suzt37ohetSKLNP0kFUv0Ji1joiXumir\n",
      "To: /Users/hendrik/Documents/M.Sc. BA - NOVA/Semester 1/Advanced Data Analytics/week 3/ada_week3_lecture2/sbe_data_2324.zip\n",
      "100%|██████████████████████████████████████| 1.07M/1.07M [00:00<00:00, 4.03MB/s]\n",
      "unzip:  cannot find or open sbe_data_2223.zip, sbe_data_2223.zip.zip or sbe_data_2223.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL ONLY IF RUNNING IN COLAB\n",
    "\n",
    "!apt-get install openjdk-11-jdk-headless\n",
    "!pip install pyspark\n",
    "!pip install gdown\n",
    "!gdown --id 1Suzt37ohetSKLNP0kFUv0Ji1joiXumir\n",
    "!unzip sbe_data_2223.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57880c7",
   "metadata": {},
   "source": [
    "## Import Spark SQL\n",
    "\n",
    "Import the Spark SQL packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69621cc6",
   "metadata": {
    "id": "69621cc6"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b34692",
   "metadata": {
    "id": "40b34692"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Simple test\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b23c33",
   "metadata": {
    "id": "c7b23c33"
   },
   "source": [
    "## Window operations\n",
    "\n",
    "SQL allows to define operations over windows of values in a table.\n",
    "\n",
    "This can be used for computing, for example, moving averages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b421d0",
   "metadata": {
    "id": "b5b421d0"
   },
   "source": [
    "Consider the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742dfdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d742dfdf",
    "outputId": "33109bec-9183-4b91-9279-0021303b45ad"
   },
   "outputs": [],
   "source": [
    "dataDF = spark.createDataFrame( [(1,10), (2,11), (3,13), (4,16), (5,20), \\\n",
    "                                 (6,25), (7,31), (8,38), (9,46), (10,55)],\n",
    "                               [\"day\",\"value\"])\n",
    "dataDF.createOrReplaceTempView(\"data0\")\n",
    "dataDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06534f22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06534f22",
    "outputId": "793de63d-c8f3-448b-a706-faf5e2df14c5"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT day, value, MEAN(value) OVER \n",
    "                            (ORDER BY day ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING)\n",
    "                            AS centerMA\n",
    "                        FROM data0\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbd2c1",
   "metadata": {
    "id": "dbdbd2c1"
   },
   "source": [
    "### Moving average\n",
    "\n",
    "The syntax for using [windows in Spark SQL](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html) is the following (simplified):\n",
    "```SELECT fun(col1) OVER (ORDER BY col2 ROWS BETWEEN offset AND offset FROM ...```, with offset ```UNBOUNDED PRECEDING | offset PRECEDING | CURRENT ROW | offset FOLLOWING | UNBOUNDED FOLLOWING```.\n",
    "\n",
    "Computing an approximation of the moving averages can be done as follows - unlike moving averages, the computation is still performed if the windows is not complete. For this reason, the computed values include results for all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4b3ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ef4b3ee",
    "outputId": "f0d1b9fa-cd8c-4d92-c174-9cf9bb3d45a4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT day, value, MEAN(value) OVER \n",
    "                            (ORDER BY day ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING)\n",
    "                            AS centerMA\n",
    "                        FROM data0\"\"\").createOrReplaceTempView(\"data1\")\n",
    "\n",
    "result = spark.sql(\"select * from data1\")\n",
    "result.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828436a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "828436a4",
    "outputId": "103bae3f-114a-4256-f206-a4661187366a"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT day, value, centerMA, MEAN(value) OVER \n",
    "                            (ORDER BY day ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n",
    "                            AS simpleMA\n",
    "                        FROM data1\n",
    "                        ORDER BY day\"\"\").createOrReplaceTempView(\"data2\")\n",
    "\n",
    "result = spark.sql(\"SELECT * FROM data2\")\n",
    "result.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c196c5",
   "metadata": {
    "id": "d1c196c5"
   },
   "source": [
    "You should have seen the warning: **No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.**. \n",
    "\n",
    "The problem with Window operations performed over distributed data is that it is not possible to partition data, as windows overlap in a rolling way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43421642",
   "metadata": {
    "id": "43421642"
   },
   "source": [
    "### Cumulative sum\n",
    "\n",
    "Other computations that can be performed easily using windows are the cummulative sum, mean, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29275273",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29275273",
    "outputId": "1edb057d-7d0c-46fa-c944-3fcbc0ed48a7"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT day, value, centerMA, simpleMA, SUM(value) OVER \n",
    "                            (ORDER BY day ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
    "                            AS acum\n",
    "                        FROM data2\n",
    "                        ORDER BY day\"\"\").createOrReplaceTempView(\"data3\")\n",
    "\n",
    "result = spark.sql(\"select * from data3\")\n",
    "result.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2f008",
   "metadata": {
    "id": "67f2f008"
   },
   "source": [
    "### Operations over groups using windows\n",
    "\n",
    "Windows can also be used to execute operation over groups, because the definition of a windows includes the possibility of partitioning the table and executing the defined operation over each partition independently.\n",
    "\n",
    "We can use this for, in the example of the first part of lecture 2, get the row with the youngest persons that are a good and bad company.\n",
    "\n",
    "Let's start by loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf9555",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ccf9555",
    "outputId": "8377aa73-05d5-4597-bf7e-176670516417"
   },
   "outputs": [],
   "source": [
    "# Let's create a PATH in a OS independent way\n",
    "# File lec1-example.csv is in directory data\n",
    "fileName = os.path.join( \"data\", \"lec1-example.csv\")\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\",True).csv(fileName)\n",
    "df = df.withColumnRenamed(\"Educational level\", \"EducationalLevel\")\n",
    "\n",
    "df.createOrReplaceTempView(\"persons\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5166f5",
   "metadata": {
    "id": "eb5166f5"
   },
   "source": [
    "The following code partitions data by the value of the Company column, and for each partition order the rows by age ```(PARTITION BY company ORDER BY age)```. For each group, it will get the position (rank) of the row in the order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3628405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3628405",
    "outputId": "01ae26fc-7d2e-413b-aeb8-6d35ae527166"
   },
   "outputs": [],
   "source": [
    "youngest = spark.sql(\"\"\"SELECT *, RANK(age) OVER (PARTITION BY company ORDER BY age) AS rank\n",
    "                            FROM persons\n",
    "                            \"\"\")\n",
    "youngest.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345e18e",
   "metadata": {
    "id": "4345e18e"
   },
   "source": [
    "Now, it is just necessary to keep only the rows with rank equals to 1. The following code creates an additional table and then performs a selection on the new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad6f6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18ad6f6c",
    "outputId": "912140b9-037e-4354-d82a-952383d44554"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT *, RANK(age) OVER (PARTITION BY company ORDER BY age) AS rank\n",
    "                            FROM persons\n",
    "                            \"\"\").createOrReplaceTempView(\"personsExt\")\n",
    "youngest = spark.sql(\"\"\"SELECT * FROM personsExt WHERE rank = 1\"\"\")\n",
    "\n",
    "youngest.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3bf00",
   "metadata": {
    "id": "02b3bf00"
   },
   "source": [
    "The following code performs the same computation by using a nested **SELECT**, where a select is performed on the result of other select statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a3564",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e54a3564",
    "outputId": "8d10445f-f182-4ea3-cfa1-8ab391817949"
   },
   "outputs": [],
   "source": [
    "youngest = spark.sql(\"\"\"SELECT * FROM (SELECT *, \n",
    "                            RANK(age) OVER (PARTITION BY company ORDER BY age) AS rank\n",
    "                        FROM persons)\n",
    "                        WHERE rank = 1\"\"\")\n",
    "youngest.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cffed57",
   "metadata": {
    "id": "0cffed57"
   },
   "source": [
    "## Correlations\n",
    "\n",
    "It is possible to use statistical function, such as correlation in Spark SQL.\n",
    "\n",
    "Let's start by computing the movign averages of COVID cases and deaths in Portugal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38ee22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "af38ee22",
    "outputId": "9805704b-bed4-4b82-c9f7-46594f634596"
   },
   "outputs": [],
   "source": [
    "ptCovidFileName = os.path.join( \"data\", \"PT-covid.csv\")\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "ptCovidDF = spark.read.option(\"header\", True).option(\"inferSchema\",True).csv(ptCovidFileName)\n",
    "\n",
    "# Converts the date into the datetime type\n",
    "ptCovidDF = ptCovidDF.withColumn(\"date\",to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "ptCovidDF.createOrReplaceTempView(\"covidPT0\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT *, MEAN(cases) OVER \n",
    "                            (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n",
    "                            AS casesMA,\n",
    "                       MEAN(deaths) OVER \n",
    "                            (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n",
    "                            AS deathsMA\n",
    "                        FROM covidPT0\"\"\").createOrReplaceTempView(\"covidPT\")\n",
    "\n",
    "ptCovidPdDF = spark.sql(\"SELECT date, casesMA, deathsMA FROM covidPT\").toPandas()\n",
    "\n",
    "ptCovidPdDF.plot(x=\"date\",y=[\"casesMA\",\"deathsMA\"],secondary_y=[\"deathsMA\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4fc2f",
   "metadata": {
    "id": "c9e4fc2f"
   },
   "source": [
    "It is now possible to compute the correlation between two columns by calling the function ```CORR( col1, col2)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65bdf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de65bdf1",
    "outputId": "770de772-29cd-4f51-dc13-f14b228e3afd"
   },
   "outputs": [],
   "source": [
    "corrDF = spark.sql(\"SELECT CORR(casesMA, deathsMA) FROM covidPT\")\n",
    "\n",
    "corrDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff393439",
   "metadata": {
    "id": "ff393439"
   },
   "source": [
    "It is possible to compute the correlation only for a subset of rows, by using a ```WHERE``` condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325b02b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2325b02b",
    "outputId": "781e6e0f-5c6b-4a4f-ef4d-34b08fadd2c9"
   },
   "outputs": [],
   "source": [
    "print(\"Before 2021-03-01\")\n",
    "corrDF = spark.sql(\"SELECT CORR(casesMA, deathsMA) FROM covidPT WHERE date < '2021-03-01'\")\n",
    "corrDF.show()\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"After 2021-03-01\")\n",
    "corrDF = spark.sql(\"SELECT CORR(casesMA, deathsMA) FROM covidPT WHERE date >= '2021-03-01'\")\n",
    "corrDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00790476",
   "metadata": {
    "id": "00790476"
   },
   "source": [
    "## Functions over multiple tables\n",
    "\n",
    "Often, data will be in multiple tables/Dataframes. To process data, it is necessary to execute operations over these tables. \n",
    "\n",
    "We now introduce some of the operations available on Spark SQL for combining multiple tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc783041",
   "metadata": {
    "id": "dc783041"
   },
   "source": [
    "### Appending tables\n",
    "\n",
    "Sometimes, we have data over which we want to perform a computation that is in two tables. \n",
    "\n",
    "The ```SELECT ... FROM table1 ... UNION SELECT ... FROM table2 ...``` allows to combine the selected rows from both tables - in general, ```UNION``` allows to combine the results from two (or more) select statements. Unlike Pandas ```append```function, results must have the same columns.\n",
    "\n",
    "The following code show the example running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ea8e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f01ea8e2",
    "outputId": "34a9a20d-6b51-424f-9ae3-2b39b1f27e00"
   },
   "outputs": [],
   "source": [
    "population1DF = spark.createDataFrame( [(\"PT\",10276617), (\"ES\",46937060), (\"DE\",83019213)],\n",
    "                               [\"country\",\"population\"])\n",
    "population1DF.createOrReplaceTempView(\"population1\")\n",
    "population1DF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d61f46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8d61f46",
    "outputId": "750216a1-65db-4029-900b-426be554c0db"
   },
   "outputs": [],
   "source": [
    "population2DF = spark.createDataFrame( [(\"BR\",211049519), \n",
    "                                        (\"MX\",127575529), \n",
    "                                        (\"UY\",3461731)],\n",
    "                               [\"country\",\"population\"])\n",
    "population2DF.createOrReplaceTempView(\"population2\")\n",
    "population2DF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c192c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "963c192c",
    "outputId": "b580ea83-c55c-4b74-f62d-c9bbf488dfd0"
   },
   "outputs": [],
   "source": [
    "population = spark.sql(\"SELECT * FROM population1 UNION select * FROM population2\")\n",
    "population.createOrReplaceTempView(\"population\")\n",
    "\n",
    "population.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c2940",
   "metadata": {
    "id": "481c2940"
   },
   "source": [
    "### Joining tables\n",
    "\n",
    "It is also possible to join two or more table. \n",
    "\n",
    "Consider thehat we have the following two tables. The first table has a list of countries and their population.\n",
    "\n",
    "| country | population |\n",
    "|---------|------------|\n",
    "| PT | 10276617 |\n",
    "| ES | 46937060 |\n",
    "| DE | 83019213 |\n",
    "\n",
    "The second table has the language spoken in each country.\n",
    "\n",
    "| country | language |\n",
    "|---------|----------|\n",
    "| PT | Portuguese |\n",
    "| ES | Spanish |\n",
    "| MX | Spanish |\n",
    "| AR | Spanish |\n",
    "| DE | German |\n",
    "| IT | Italian |\n",
    "| BR | Portuguese |\n",
    "\n",
    "If we want to compute the number of persons that speak each language, it would be interesting to have a single table with the country, population and language columns. To this end, we need to combine both of the previous tables (this can also be seen as extending the first table with the values of the second table).\n",
    "\n",
    "What we want to achieve is the following table, with columns country, population and language: \n",
    "\n",
    "| country | population | language |\n",
    "|---------|------------|----------|\n",
    "| PT | 10276617 | Portuguese |\n",
    "| ES | 46937060 | Spanish |\n",
    "| DE | 83019213 | German |\n",
    "\n",
    "\n",
    "The ```SELECT * FROM table1 [LEFT | INNER | RIGHT | OUTTER | nothing] JOIN table2 ON condition ....``` joins two tables using the given condition to specify how a row in table1 is joined with rows in table2. By default, join performs an INNER join.\n",
    "\n",
    "In our example, we want to combine the rows with the same country.\n",
    "\n",
    "This example is coded in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47752e0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47752e0e",
    "outputId": "4d6bae2b-412d-4a0b-ccd0-47176d1c0103"
   },
   "outputs": [],
   "source": [
    "languageDF = spark.createDataFrame( [(\"PT\",\"Portuguese\"), (\"ES\",\"Spanish\"), (\"MX\",\"Spanish\"), \n",
    "                                     (\"AR\",\"Spanish\"), (\"DE\",\"German\"), (\"BR\",\"Portuguese\")],\n",
    "                               [\"country\",\"language\"])\n",
    "languageDF.createOrReplaceTempView(\"language\")\n",
    "languageDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceaf588",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fceaf588",
    "outputId": "876d007c-ca3c-4d41-eea6-5d15b0a17d94"
   },
   "outputs": [],
   "source": [
    "countries1 = spark.sql(\"\"\"SELECT * FROM population1 \n",
    "                                JOIN language ON population1.country = language.country\"\"\")\n",
    "\n",
    "countries1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca3e08",
   "metadata": {
    "id": "14ca3e08"
   },
   "source": [
    "Sometimes, it is convenient to use a shorter table name to refer to columns - this can be done by usins a ```table AS short_name``` or simply ```table short_name```. \n",
    "\n",
    "The following code uses this and gets a single entry for the country.\n",
    "\n",
    "NOTE: you only need to prefix a column name with the name of the table when confusion may arise. In the following code, as population and language columns only occur in one of the table, there is no need to prefix references with the table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac23fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6ac23fd",
    "outputId": "ac148285-3e7c-4849-f694-b071249df68c"
   },
   "outputs": [],
   "source": [
    "countries1 = spark.sql(\"\"\"SELECT p.country, population, language FROM population1 p \n",
    "                                    JOIN language l ON p.country = l.country\"\"\")\n",
    "\n",
    "countries1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88930e6",
   "metadata": {
    "id": "a88930e6"
   },
   "source": [
    "It is possible to execute any function over the values of joined table. The following code computes the number of persons that speak each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceecc1e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceecc1e0",
    "outputId": "bc731b69-c0f2-42d5-86fa-38ab9506b79c"
   },
   "outputs": [],
   "source": [
    "langStats = spark.sql(\"\"\"SELECT language, SUM(population) AS population \n",
    "                                FROM population1 p \n",
    "                                JOIN language l ON p.country = l.country \n",
    "                                GROUP BY language\"\"\")\n",
    "langStats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7d58c",
   "metadata": {
    "id": "a9d7d58c"
   },
   "source": [
    "### Join type : left\n",
    "\n",
    "The way join works varies depending on the type of join.\n",
    "\n",
    "In a left join, each row of *table1* is combined with all possible values of *table2*. If no row in the second table matches the joining column of the first, then the value for the columns will be **null**.\n",
    "\n",
    "For better exemplifying, we start by extending our language table to include one other language for Spain : Catalan.\n",
    "\n",
    "| country | language |\n",
    "|---------|----------|\n",
    "| PT | Portuguese |\n",
    "| ES | Spanish |\n",
    "| ES | Catalan |\n",
    "| MX | Spanish |\n",
    "| AR | Spanish |\n",
    "| DE | German |\n",
    "| IT | Italian |\n",
    "| BR | Portuguese |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1242faa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1242faa",
    "outputId": "7ef5b78f-e1fe-4876-8d5f-40371b65be3f"
   },
   "outputs": [],
   "source": [
    "languageExtDF = spark.createDataFrame( [(\"PT\",\"Portuguese\"), (\"ES\",\"Spanish\"), (\"ES\",\"Catalan\"), (\"MX\",\"Spanish\"), \n",
    "                                     (\"AR\",\"Spanish\"), (\"DE\",\"German\"), (\"IT\",\"Italian\"), (\"BR\",\"Portuguese\")],\n",
    "                               [\"country\",\"language\"])\n",
    "languageExtDF.createOrReplaceTempView(\"languageExt\")\n",
    "languageExtDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a60261",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29a60261",
    "outputId": "42760888-fddd-4ddf-cd9d-bd966cd07540"
   },
   "outputs": [],
   "source": [
    "countries = spark.sql(\"\"\"SELECT * FROM population p LEFT JOIN language l\n",
    "                                ON p.country = l.country\"\"\")\n",
    "\n",
    "countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552476a",
   "metadata": {
    "id": "5552476a"
   },
   "source": [
    "The line for Uruguay (UY) has **null** in the language column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35d405",
   "metadata": {
    "id": "2b35d405"
   },
   "source": [
    "### Join type : right\n",
    "\n",
    "In a right join, each row of *table* is combined with all possible values in the first *table*. If no row in the first table matches the joining column of the second, then the value for the columns will be **null**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ad906",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b5ad906",
    "outputId": "f9224086-76d1-49e1-9e4b-1bf57822c2b9"
   },
   "outputs": [],
   "source": [
    "countries = spark.sql(\"\"\"SELECT * FROM population p RIGHT JOIN language l\n",
    "                                ON p.country = l.country\"\"\")\n",
    "\n",
    "countries.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59400c03",
   "metadata": {
    "id": "59400c03"
   },
   "source": [
    "### Join type : inner (default)\n",
    "\n",
    "In an inner join, each row of *table1* is combined with all possible values in the second *table*. If no row in the second table matches the joining column of the first, then the row will not be part of the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357bfd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8357bfd9",
    "outputId": "67899f52-6332-4db8-d4e7-20cf0000c77e"
   },
   "outputs": [],
   "source": [
    "countries = spark.sql(\"\"\"SELECT * FROM population p INNER JOIN language l\n",
    "                                ON p.country = l.country\"\"\")\n",
    "countries.show()\n",
    "\n",
    "# Just check that INNER is the default mode\n",
    "countries = spark.sql(\"\"\"SELECT * FROM population p JOIN language l\n",
    "                                ON p.country = l.country\"\"\")\n",
    "countries.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126348d",
   "metadata": {
    "id": "3126348d"
   },
   "source": [
    "### Join type : outer\n",
    "\n",
    "In an outter join, each row of *table1* is combined with all possible values in the second *table*. If no row exists in any of the dataframes, both row will appear in the final result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c1387",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "109c1387",
    "outputId": "859b277d-4ab4-4477-a03f-707a9bf216df"
   },
   "outputs": [],
   "source": [
    "countries = spark.sql(\"\"\"SELECT * FROM population p FULL OUTER JOIN language l\n",
    "                                ON p.country = l.country\"\"\")\n",
    "countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a019da",
   "metadata": {
    "id": "d9a019da"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "File **countries.csv** has information on countries, including the continent.\n",
    "\n",
    "Using this file and compute and plot statistics on cases and deaths by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858712e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "858712e8",
    "outputId": "66ad776d-b003-4a01-932a-1a58a27c56b9"
   },
   "outputs": [],
   "source": [
    "countriesInfoFileName = os.path.join( \"data\", \"countries.csv\")\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "countriesInfoDF = spark.read.option(\"header\", True).option(\"inferSchema\",True).csv(countriesInfoFileName)\n",
    "\n",
    "## Register Dataframe as a temporary view\n",
    "countriesInfoDF.createOrReplaceTempView(\"countriesInfo\")\n",
    "\n",
    "countriesInfoDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b916263",
   "metadata": {
    "id": "9b916263"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Based on the data in the **ALL-covid.csv** and **countries.csv**, compute and plot information on COVID by continent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ea070",
   "metadata": {
    "id": "6f9ea070"
   },
   "outputs": [],
   "source": [
    "allCovidFileName = os.path.join( \"data\", \"ALL-covid.csv\")\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "allCovidDF = spark.read.option(\"header\", True).option(\"inferSchema\",True).csv(allCovidFileName)\n",
    "\n",
    "# Converts the date into the datetime type\n",
    "allCovidDF = allCovidDF.withColumn(\"date\",to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "allCovidDF.createOrReplaceTempView(\"covidALL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89bf4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "9f89bf4e",
    "outputId": "ce85a207-a7b6-49e9-e6da-fefbcc9d1483"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0bf2b",
   "metadata": {
    "id": "71e0bf2b"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Select at least four countries - find which pair of countries have shown a more direct correlation for the cases of COVID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61511d9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61511d9d",
    "outputId": "8e797be0-320a-41e5-b432-d4f3586df391"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8b314",
   "metadata": {
    "id": "03b8b314"
   },
   "outputs": [],
   "source": [
    "#Let's stop the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5648e",
   "metadata": {
    "id": "6cc5648e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ADA_week6_COLAB_lecture2_part2-Solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
