{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e90bac",
   "metadata": {
    "id": "d9e90bac"
   },
   "source": [
    "# Advanced Data Analysis - week 3, lecture 1, examples\n",
    "\n",
    "In the advanced data analysis course, we assume basic knowledge of Python, as could be acquired by attending the *Introduction to Programming* bridging course.\n",
    "\n",
    "This notebook includes the examples and exercises presented in **Week 3**, lecture 1. There is an additional notebook with the examples and exercises suggested for autonomous study during the week.\n",
    "\n",
    "In **week 3**, we will focus on introducing Spark.\n",
    "\n",
    "**This notebook should be run on Google Colab**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HVBs4DLPSBvs",
   "metadata": {
    "id": "HVBs4DLPSBvs"
   },
   "source": [
    "## Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02hiwXn1SA7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02hiwXn1SA7c",
    "outputId": "4520ac86-3af5-4c78-ef6a-e9b722ad0b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "openjdk-11-jdk-headless is already the newest version (11.0.20.1+1-0ubuntu1~22.04).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=1e135567cff58f8edd53dda85b1152385ed8746accd2e8f9b569cbc8426e5a26\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.4.1\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Access denied with the following error:\n",
      "\n",
      " \tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=0-tJ7cJ3wRHCnUkHiKVFQzFQ \n",
      "\n",
      "unzip:  cannot find or open sbe_data_2324.zip, sbe_data_2324.zip.zip or sbe_data_2324.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL ONLY IF RUNNING IN COLAB\n",
    "\n",
    "!apt-get install openjdk-11-jdk-headless\n",
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9RjI4_vLbvVn",
   "metadata": {
    "id": "9RjI4_vLbvVn"
   },
   "source": [
    "### Downloading data files\n",
    "\n",
    "This cell will download the dataset files used in the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eRDUUjVSbQk-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRDUUjVSbQk-",
    "outputId": "3f4360df-9942-4198-94a8-a2bc4d0051dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "/usr/local/lib/python3.10/dist-packages/gdown/parse_url.py:35: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=1Suzt37ohetSKLNP0kFUv0Ji1joiXumir\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/file/d/1Suzt37ohetSKLNP0kFUv0Ji1joiXumir/view?usp=sharing\n",
      "To: /content/view?usp=sharing\n",
      "80.5kB [00:00, 4.76MB/s]\n",
      "Archive:  sbe_data_2324.zip\n",
      "  inflating: data/AD-covid.csv       \n",
      "  inflating: data/AE-covid.csv       \n",
      "  inflating: data/AF-covid.csv       \n",
      "  inflating: data/AG-covid.csv       \n",
      "  inflating: data/AL-covid.csv       \n",
      "  inflating: data/ALL-covid.csv      \n",
      "  inflating: data/AM-covid.csv       \n",
      "  inflating: data/AO-covid.csv       \n",
      "  inflating: data/AR-covid.csv       \n",
      "  inflating: data/AT-covid.csv       \n",
      "  inflating: data/AU-covid.csv       \n",
      "  inflating: data/AW-covid.csv       \n",
      "  inflating: data/AZ-covid.csv       \n",
      "  inflating: data/BA-covid.csv       \n",
      "  inflating: data/BB-covid.csv       \n",
      "  inflating: data/BD-covid.csv       \n",
      "  inflating: data/BE-covid.csv       \n",
      "  inflating: data/BF-covid.csv       \n",
      "  inflating: data/BG-covid.csv       \n",
      "  inflating: data/BH-covid.csv       \n",
      "  inflating: data/BI-covid.csv       \n",
      "  inflating: data/BJ-covid.csv       \n",
      "  inflating: data/BM-covid.csv       \n",
      "  inflating: data/BN-covid.csv       \n",
      "  inflating: data/BO-covid.csv       \n",
      "  inflating: data/BR-covid.csv       \n",
      "  inflating: data/BS-covid.csv       \n",
      "  inflating: data/BT-covid.csv       \n",
      "  inflating: data/BW-covid.csv       \n",
      "  inflating: data/BY-covid.csv       \n",
      "  inflating: data/BZ-covid.csv       \n",
      "  inflating: data/CA-covid.csv       \n",
      "  inflating: data/CD-covid.csv       \n",
      "  inflating: data/CF-covid.csv       \n",
      "  inflating: data/CG-covid.csv       \n",
      "  inflating: data/CH-covid.csv       \n",
      "  inflating: data/CI-covid.csv       \n",
      "  inflating: data/CL-covid.csv       \n",
      "  inflating: data/CM-covid.csv       \n",
      "  inflating: data/CN-covid.csv       \n",
      "  inflating: data/CO-covid.csv       \n",
      "  inflating: data/CR-covid.csv       \n",
      "  inflating: data/CU-covid.csv       \n",
      "  inflating: data/CV-covid.csv       \n",
      "  inflating: data/CY-covid.csv       \n",
      "  inflating: data/CZ-covid.csv       \n",
      "  inflating: data/DE-covid.csv       \n",
      "  inflating: data/DJ-covid.csv       \n",
      "  inflating: data/DK-covid.csv       \n",
      "  inflating: data/DM-covid.csv       \n",
      "  inflating: data/DO-covid.csv       \n",
      "  inflating: data/DZ-covid.csv       \n",
      "  inflating: data/EC-covid.csv       \n",
      "  inflating: data/EE-covid.csv       \n",
      "  inflating: data/EG-covid.csv       \n",
      "  inflating: data/ER-covid.csv       \n",
      "  inflating: data/ES-covid.csv       \n",
      "  inflating: data/ET-covid.csv       \n",
      "  inflating: data/FI-covid.csv       \n",
      "  inflating: data/FJ-covid.csv       \n",
      "  inflating: data/FO-covid.csv       \n",
      "  inflating: data/FR-covid.csv       \n",
      "  inflating: data/GA-covid.csv       \n",
      "  inflating: data/GB-covid.csv       \n",
      "  inflating: data/GD-covid.csv       \n",
      "  inflating: data/GE-covid.csv       \n",
      "  inflating: data/GH-covid.csv       \n",
      "  inflating: data/GL-covid.csv       \n",
      "  inflating: data/GM-covid.csv       \n",
      "  inflating: data/GN-covid.csv       \n",
      "  inflating: data/GQ-covid.csv       \n",
      "  inflating: data/GR-covid.csv       \n",
      "  inflating: data/GT-covid.csv       \n",
      "  inflating: data/GU-covid.csv       \n",
      "  inflating: data/GW-covid.csv       \n",
      "  inflating: data/GY-covid.csv       \n",
      "  inflating: data/HK-covid.csv       \n",
      "  inflating: data/HN-covid.csv       \n",
      "  inflating: data/HR-covid.csv       \n",
      "  inflating: data/HT-covid.csv       \n",
      "  inflating: data/HU-covid.csv       \n",
      "  inflating: data/ID-covid.csv       \n",
      "  inflating: data/IE-covid.csv       \n",
      "  inflating: data/IL-covid.csv       \n",
      "  inflating: data/IN-covid.csv       \n",
      "  inflating: data/IQ-covid.csv       \n",
      "  inflating: data/IR-covid.csv       \n",
      "  inflating: data/IS-covid.csv       \n",
      "  inflating: data/IT-covid.csv       \n",
      "  inflating: data/JM-covid.csv       \n",
      "  inflating: data/JO-covid.csv       \n",
      "  inflating: data/JP-covid.csv       \n",
      "  inflating: data/KE-covid.csv       \n",
      "  inflating: data/KG-covid.csv       \n",
      "  inflating: data/KH-covid.csv       \n",
      "  inflating: data/KI-covid.csv       \n",
      "  inflating: data/KM-covid.csv       \n",
      "  inflating: data/KN-covid.csv       \n",
      "  inflating: data/KR-covid.csv       \n",
      "  inflating: data/KW-covid.csv       \n",
      "  inflating: data/KZ-covid.csv       \n",
      "  inflating: data/LA-covid.csv       \n",
      "  inflating: data/LB-covid.csv       \n",
      "  inflating: data/LC-covid.csv       \n",
      "  inflating: data/LI-covid.csv       \n",
      "  inflating: data/LK-covid.csv       \n",
      "  inflating: data/LR-covid.csv       \n",
      "  inflating: data/LS-covid.csv       \n",
      "  inflating: data/LT-covid.csv       \n",
      "  inflating: data/LU-covid.csv       \n",
      "  inflating: data/LV-covid.csv       \n",
      "  inflating: data/LY-covid.csv       \n",
      "  inflating: data/MA-covid.csv       \n",
      "  inflating: data/MC-covid.csv       \n",
      "  inflating: data/MD-covid.csv       \n",
      "  inflating: data/ME-covid.csv       \n",
      "  inflating: data/MG-covid.csv       \n",
      "  inflating: data/MH-covid.csv       \n",
      "  inflating: data/ML-covid.csv       \n",
      "  inflating: data/MM-covid.csv       \n",
      "  inflating: data/MN-covid.csv       \n",
      "  inflating: data/MO-covid.csv       \n",
      "  inflating: data/MR-covid.csv       \n",
      "  inflating: data/MT-covid.csv       \n",
      "  inflating: data/MU-covid.csv       \n",
      "  inflating: data/MV-covid.csv       \n",
      "  inflating: data/MW-covid.csv       \n",
      "  inflating: data/MX-covid.csv       \n",
      "  inflating: data/MY-covid.csv       \n",
      "  inflating: data/MZ-covid.csv       \n",
      "  inflating: data/NA-covid.csv       \n",
      "  inflating: data/NE-covid.csv       \n",
      "  inflating: data/NG-covid.csv       \n",
      "  inflating: data/NI-covid.csv       \n",
      "  inflating: data/NL-covid.csv       \n",
      "  inflating: data/NO-covid.csv       \n",
      "  inflating: data/NP-covid.csv       \n",
      "  inflating: data/NZ-covid.csv       \n",
      "  inflating: data/OM-covid.csv       \n",
      "  inflating: data/PA-covid.csv       \n",
      "  inflating: data/PE-covid.csv       \n",
      "  inflating: data/PG-covid.csv       \n",
      "  inflating: data/PH-covid.csv       \n",
      "  inflating: data/PK-covid.csv       \n",
      "  inflating: data/PL-covid.csv       \n",
      "  inflating: data/PR-covid.csv       \n",
      "  inflating: data/PS-covid.csv       \n",
      "  inflating: data/PT-covid.csv       \n",
      "  inflating: data/PW-covid.csv       \n",
      "  inflating: data/PY-covid.csv       \n",
      "  inflating: data/QA-covid.csv       \n",
      "  inflating: data/RK-covid.csv       \n",
      "  inflating: data/RO-covid.csv       \n",
      "  inflating: data/RS-covid.csv       \n",
      "  inflating: data/RU-covid.csv       \n",
      "  inflating: data/RW-covid.csv       \n",
      "  inflating: data/SA-covid.csv       \n",
      "  inflating: data/SB-covid.csv       \n",
      "  inflating: data/SC-covid.csv       \n",
      "  inflating: data/SD-covid.csv       \n",
      "  inflating: data/SE-covid.csv       \n",
      "  inflating: data/SG-covid.csv       \n",
      "  inflating: data/SI-covid.csv       \n",
      "  inflating: data/SK-covid.csv       \n",
      "  inflating: data/SL-covid.csv       \n",
      "  inflating: data/SM-covid.csv       \n",
      "  inflating: data/SN-covid.csv       \n",
      "  inflating: data/SO-covid.csv       \n",
      "  inflating: data/SR-covid.csv       \n",
      "  inflating: data/SS-covid.csv       \n",
      "  inflating: data/ST-covid.csv       \n",
      "  inflating: data/SV-covid.csv       \n",
      "  inflating: data/SY-covid.csv       \n",
      "  inflating: data/SZ-covid.csv       \n",
      "  inflating: data/TD-covid.csv       \n",
      "  inflating: data/TG-covid.csv       \n",
      "  inflating: data/TH-covid.csv       \n",
      "  inflating: data/TJ-covid.csv       \n",
      "  inflating: data/TL-covid.csv       \n",
      "  inflating: data/TN-covid.csv       \n",
      "  inflating: data/TR-covid.csv       \n",
      "  inflating: data/TT-covid.csv       \n",
      "  inflating: data/TW-covid.csv       \n",
      "  inflating: data/TZ-covid.csv       \n",
      "  inflating: data/UA-covid.csv       \n",
      "  inflating: data/UG-covid.csv       \n",
      "  inflating: data/US-covid.csv       \n",
      "  inflating: data/UY-covid.csv       \n",
      "  inflating: data/UZ-covid.csv       \n",
      "  inflating: data/VC-covid.csv       \n",
      "  inflating: data/VE-covid.csv       \n",
      "  inflating: data/VI-covid.csv       \n",
      "  inflating: data/VN-covid.csv       \n",
      "  inflating: data/VU-covid.csv       \n",
      "  inflating: data/WS-covid.csv       \n",
      "  inflating: data/XK-covid.csv       \n",
      "  inflating: data/YE-covid.csv       \n",
      "  inflating: data/ZA-covid.csv       \n",
      "  inflating: data/ZM-covid.csv       \n",
      "  inflating: data/ZW-covid.csv       \n",
      "  inflating: data/lec1-example.csv   \n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/file/d/1Suzt37ohetSKLNP0kFUv0Ji1joiXumir/view?usp=sharing\n",
    "!unzip -o sbe_data_2324.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f57801",
   "metadata": {
    "id": "b6f57801"
   },
   "source": [
    "## Programming (with Pandas API for Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aac495",
   "metadata": {
    "id": "d3aac495"
   },
   "source": [
    "We now show how to program with the [**Pandas API**](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) for Spark.\n",
    "\n",
    "For using Pandas, you must import *pyspark.pandas*. After that, you can use the Pandas API, but it will run on Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69621cc6",
   "metadata": {
    "id": "69621cc6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "# imports pandas API for Spark\n",
    "import pyspark.pandas as ps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d7e68",
   "metadata": {
    "id": "e95d7e68"
   },
   "source": [
    "### Data model : DataFrame\n",
    "\n",
    "In *Pandas* API, a table is represented as a [**DataFrame**](https://pandas.pydata.org/docs/reference/frame.html), using the underlying Spark DataFrame. (follow the link for DataFrame documentation)\n",
    "\n",
    "There are multiple ways to create an initial DataFrame. For example, you can create date from a Python dictionary, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01ea8e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f01ea8e2",
    "outputId": "3787d210-c061-4ea1-ed6d-bd7c557d06ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  population\n",
      "0      PT    10276617\n",
      "1      ES    46937060\n",
      "2      DE    83019213\n"
     ]
    }
   ],
   "source": [
    "population = ps.DataFrame( { \"country\": [\"PT\", \"ES\", \"DE\"] , \\\n",
    "                            \"population\": [10276617, 46937060, 83019213]})\n",
    "\n",
    "print( population)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5703d67",
   "metadata": {
    "id": "e5703d67"
   },
   "source": [
    "Pandas will maintain an additional column, the index, with a increasing integer. This column - the first column when printing the dataframe - is created automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a985f4a",
   "metadata": {
    "id": "2a985f4a"
   },
   "source": [
    "#### Loading DataFrame from CSV files\n",
    "\n",
    "More often, will want to load the data from files. To create a DataFrame from a CSV file, you can use the ```load_csv``` function.\n",
    "\n",
    "Note: If the following code fails, the most likely reason is that you do not have the *data* directory with the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3da4bdbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3da4bdbc",
    "outputId": "953c1578-57a1-4761-b0e4-98d9a153a32d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Name  Age  Educational level Company\n",
      "0     Andrew   55                1.0    Good\n",
      "1   Bernhard   43                2.0    Good\n",
      "2   Carolina   37                5.0     Bad\n",
      "3     Dennis   82                3.0    Good\n",
      "4        Eve   23                3.2     Bad\n",
      "5       Fred   46                5.0    Good\n",
      "6    Gwyneth   38                4.2     Bad\n",
      "7     Hayden   50                4.0     Bad\n",
      "8      Irene   29                4.5     Bad\n",
      "9      James   42                4.1    Good\n",
      "10     Kevin   35                4.5     Bad\n",
      "11       Lea   38                2.5    Good\n",
      "12    Marcus   31                4.8     Bad\n",
      "13     Nigel   71                2.3    Good\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Let's create a PATH in a OS independent way\n",
    "# File lec1-example.csv is in directory data\n",
    "fileName = os.path.join( \"data\", \"lec1-example.csv\")\n",
    "\n",
    "# Read a CSV file into a DataFrame\n",
    "df = ps.read_csv(fileName)\n",
    "\n",
    "print( df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d453bc",
   "metadata": {
    "id": "27d453bc"
   },
   "source": [
    "#### Saving DataFrame into CSV files\n",
    "\n",
    "You can save a DataFrame into a CSV file using ```to_csv``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3249bd35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3249bd35",
    "outputId": "4556ca9d-9665-4ab1-d072-b524ca8cc9a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Let's create a PATH in a OS independent way\n",
    "# File lec1-saved.csv will be in directory data\n",
    "fileName = os.path.join( \"data\", \"lec5-saved.csv\")\n",
    "\n",
    "# Save DataFrameRead a CSV file into a DataFrame\n",
    "df.to_csv( fileName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd810e",
   "metadata": {
    "id": "61cd810e"
   },
   "source": [
    "Please check the file created. Is it the same as the original lec1-saved.csv?\n",
    "\n",
    "No, it has an additional column with the row number. You can also save the DataFrame without this number by using the ```index=False``` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772fb54e",
   "metadata": {
    "id": "772fb54e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Let's create a PATH in a OS independent way\n",
    "# File lec1-saved-noindex.csv will be in directory data\n",
    "fileName = os.path.join( \"data\", \"lec5-saved-noindex.csv\")\n",
    "\n",
    "# Save DataFrameRead a CSV file into a DataFrame\n",
    "df.to_csv( fileName, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece8b1d",
   "metadata": {
    "id": "eece8b1d"
   },
   "source": [
    "### Data processing with Pandas\n",
    "\n",
    "We now show the transformations necessary to perform the exercises proposed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae30ef2",
   "metadata": {
    "id": "4ae30ef2"
   },
   "source": [
    "#### Selecting rows based on conditions\n",
    "\n",
    "It is possible to select the rows for which a column has a given value as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e21d039",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e21d039",
    "outputId": "3a183edb-d71f-43a4-b68e-47f688057e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Name  Age  Educational level Company\n",
      "0     Andrew   55                1.0    Good\n",
      "1   Bernhard   43                2.0    Good\n",
      "3     Dennis   82                3.0    Good\n",
      "5       Fred   46                5.0    Good\n",
      "9      James   42                4.1    Good\n",
      "11       Lea   38                2.5    Good\n",
      "13     Nigel   71                2.3    Good\n"
     ]
    }
   ],
   "source": [
    "# Select the persons that are good company.\n",
    "good = df[df[\"Company\"]==\"Good\"]\n",
    "\n",
    "print(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a778b139",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a778b139",
    "outputId": "f7aeef20-0717-46e8-a1a2-043e33e51268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name  Age  Educational level Company\n",
      "3  Dennis   82                3.0    Good\n",
      "5    Fred   46                5.0    Good\n",
      "9   James   42                4.1    Good\n"
     ]
    }
   ],
   "source": [
    "# Select the persons that are good company and have educational level larger than 3.\n",
    "goodEd3plus = df[(df[\"Company\"]==\"Good\") & (df[\"Educational level\"]>=3.0)]\n",
    "\n",
    "print(goodEd3plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e8d1f",
   "metadata": {
    "id": "ab0e8d1f"
   },
   "source": [
    "#### Selecting a subset of the columns\n",
    "\n",
    "Often, we do not need all data that is in a table. We can get rid of the data we do not need by selecting the columns we want using the following syntax ```dataframe[[col1,col2,...]]```.\n",
    "\n",
    "In the following example we create a new DataFrame containing only the Name and Age columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6127b277",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6127b277",
    "outputId": "fd2e6d6c-dcae-4369-a319-4917a2e309b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Name  Age\n",
      "0     Andrew   55\n",
      "1   Bernhard   43\n",
      "2   Carolina   37\n",
      "3     Dennis   82\n",
      "4        Eve   23\n",
      "5       Fred   46\n",
      "6    Gwyneth   38\n",
      "7     Hayden   50\n",
      "8      Irene   29\n",
      "9      James   42\n",
      "10     Kevin   35\n",
      "11       Lea   38\n",
      "12    Marcus   31\n",
      "13     Nigel   71\n"
     ]
    }
   ],
   "source": [
    "# Select the persons that are good company.\n",
    "person_age = df[[\"Name\",\"Age\"]]\n",
    "\n",
    "print(person_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3d2fc",
   "metadata": {
    "id": "90b3d2fc"
   },
   "source": [
    "#### Applying reduce/aggregation functions\n",
    "\n",
    "Pandas allow to compute the reduction/aggregation for the values of one or multiple columns.\n",
    "\n",
    "You must select the columns for which you want to perform the computation, and then call the reduce/aggregation function.\n",
    "\n",
    "The following example computes first, the minimum age (```min```function), and then the minimum of both *Age* and *Educational level* at the same time. Pandas has multiple useful aggregation functions, including, maximum (```max```), minimum (```min```), mean (```mean```), median (```median```), standard deviation (```std```), etc. - check the [**DataFrame** documentation](https://pandas.pydata.org/docs/reference/frame.html) for the list of available functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "865e51fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "865e51fc",
    "outputId": "8b50c45b-cf32-4a2d-8686-4957def2760a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum age is \n",
      "38\n",
      "Minimum information for several columns now\n",
      "Age                  38.0\n",
      "Educational level     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "minAge = good[\"Age\"].min()\n",
    "print( \"Minimum age is \")\n",
    "print( minAge)\n",
    "\n",
    "mins = good[[\"Age\",\"Educational level\"]].min()\n",
    "print( \"Minimum information for several columns now\")\n",
    "print( mins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f1e58",
   "metadata": {
    "id": "696f1e58"
   },
   "source": [
    "Wait, this was not what we wanted in the first place - we want the information about the youngest person that is a good company.\n",
    "\n",
    "Function ```nsmallest(num elems, columns)``` allow to compute that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "857c0251",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "857c0251",
    "outputId": "0a9ade76-88a7-4f6a-b310-5483766d3a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name  Age  Educational level Company\n",
      "11  Lea   38                2.5    Good\n"
     ]
    }
   ],
   "source": [
    "youngestGood = good.nsmallest(1,[\"Age\"])\n",
    "print( good.nsmallest(1,[\"Age\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1674b54",
   "metadata": {
    "id": "f1674b54"
   },
   "source": [
    "#### Applying reduce/aggregation functions per group\n",
    "\n",
    "```groupby([cols])``` allows to group elements of a DataFrame before applying an aggregation function to each of the groups.\n",
    "\n",
    "The following example computes the lowest age for each value of Company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17ce4e10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17ce4e10",
    "outputId": "05680565-358f-46d4-8344-1ce8183b6058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Age\n",
      "Company     \n",
      "Good      38\n",
      "Bad       23\n",
      "The youngest person is Bad\n"
     ]
    }
   ],
   "source": [
    "youngest = df[[\"Age\",\"Company\"]].groupby([\"Company\"]).min()\n",
    "print( youngest)\n",
    "\n",
    "youngestAny = youngest.idxmin()\n",
    "print( \"The youngest person is \" + youngestAny[\"Age\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b1e41",
   "metadata": {
    "id": "2a1b1e41"
   },
   "source": [
    "#### Access Spark Dataframes from Pandas Spark Dataframe\n",
    "\n",
    "Spark Dataframe has a different API from Spark Pandas API. It is possible to expose a Pandas Dataframe as a Spark native Dataframe using *to_spark* function.\n",
    "\n",
    "For printing data in a Spark Dataframe, function *show* is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6016c0e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6016c0e5",
    "outputId": "45537111-14ba-49ed-e002-962b0b3c5400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Educational level: double (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "dfSDF = df.to_spark()\n",
    "\n",
    "dfSDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44728026",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44728026",
    "outputId": "b386d5db-105d-4191-f3e5-6265d703b808"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Age\n",
      "Company     \n",
      "Good      38\n",
      "Bad       23\n"
     ]
    }
   ],
   "source": [
    "fileName = os.path.join( \"data\", \"lec1-example.csv\")\n",
    "\n",
    "df = ps.read_csv(fileName)\n",
    "\n",
    "youngest = df[[\"Age\",\"Company\"]].groupby([\"Company\"]).min()\n",
    "\n",
    "print( youngest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2aa30f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2aa30f7",
    "outputId": "164637b0-ef8e-41d9-a746-c0252535157c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|index|Age|\n",
      "+-----+---+\n",
      "| Good| 38|\n",
      "|  Bad| 23|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "youngestSDF = youngest.to_spark(index_col='index')\n",
    "\n",
    "youngestSDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6ac12",
   "metadata": {
    "id": "aaa6ac12"
   },
   "source": [
    "An interesting function is *explain* that shows the computations that are performed in the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5176373",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5176373",
    "outputId": "504ebe7c-c3aa-4946-fc4b-26ba474a3fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[__index_level_0__#587], functions=[min(Age#550)])\n",
      "   +- Exchange hashpartitioning(__index_level_0__#587, 200), ENSURE_REQUIREMENTS, [plan_id=605]\n",
      "      +- HashAggregate(keys=[__index_level_0__#587], functions=[partial_min(Age#550)])\n",
      "         +- Project [Company#552 AS __index_level_0__#587, Age#550]\n",
      "            +- Filter atleastnnonnulls(1, Company#552)\n",
      "               +- FileScan csv [Age#550,Company#552] Batched: false, DataFilters: [atleastnnonnulls(1, Company#552)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/data/lec1-example.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:int,Company:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "youngestSDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TVLkXoatY9Gb",
   "metadata": {
    "id": "TVLkXoatY9Gb"
   },
   "source": [
    "In the following text, that has the operations to be executed (from the bottom to the top), it is possible to observe that Spark will execute the **min** agregation by first computing the **partial_min** for each group in each partition, and only after that exchange data among partitions (Exchange line) for computing the minimum for each group. Note that this approach is much more efficient than propagating all information among partitions, as much less data is propagated.\n",
    "\n",
    "Now let's see another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8sSw6HgUemsR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "8sSw6HgUemsR",
    "outputId": "60c96232-18ff-4f86-a413-7191ad25e9a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>53.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bad</th>\n",
       "      <td>34.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age\n",
       "Company           \n",
       "Good     53.857143\n",
       "Bad      34.714286"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Age\",\"Company\"]].groupby([\"Company\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a7271db-bc5a-49e4-b9e1-0513bce75496",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a7271db-bc5a-49e4-b9e1-0513bce75496",
    "outputId": "61ae0bb2-5d6e-4ee7-f175-2a52b534dff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[__index_level_0__#892], functions=[avg(Age#550)])\n",
      "   +- Exchange hashpartitioning(__index_level_0__#892, 200), ENSURE_REQUIREMENTS, [plan_id=1015]\n",
      "      +- HashAggregate(keys=[__index_level_0__#892], functions=[partial_avg(Age#550)])\n",
      "         +- Project [Company#552 AS __index_level_0__#892, Age#550]\n",
      "            +- Filter (atleastnnonnulls(1, Company#552) AND CASE WHEN CASE WHEN isnull((Company#552 = Good)) THEN false ELSE isnull((Company#552 = Good)) END THEN false ELSE CASE WHEN isnull((Company#552 = Good)) THEN false ELSE (Company#552 = Good) END END)\n",
      "               +- FileScan csv [Age#550,Company#552] Batched: false, DataFilters: [atleastnnonnulls(1, Company#552), CASE WHEN CASE WHEN isnull((Company#552 = Good)) THEN false EL..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/data/lec1-example.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:int,Company:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "means = df[[\"Age\",\"Company\"]].groupby([\"Company\"]).mean().reset_index()\n",
    "meanGood = means[means[\"Company\"]==\"Good\"]\n",
    "meanGood.to_spark().explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26c5ec-b294-49ac-98f5-5dcba2d2c5db",
   "metadata": {
    "id": "9a26c5ec-b294-49ac-98f5-5dcba2d2c5db"
   },
   "source": [
    "The explanation should be read from bottom to top, starting with FileScan that represent the read of data from the file. In this example, you can see that, even if in the program the filtering is the last operation ```meanGood = means[means[\"Company\"]==\"Good\"]```, Spark will start by filtering lines for which the company is not good when reading from the file.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because there is no point in computing the number of persons that are not a good company if in the end, everyhting is needed is for those that are a Good company. This kind of optimization can have a great impact on program execution when data is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaeda31",
   "metadata": {
    "id": "ccaeda31"
   },
   "source": [
    "#### Creating Pandas Dataframes from Pandas Spark Dataframe\n",
    "\n",
    "It is possible to create a *plain* Pandas Dataframe from a Pandas Spark Dataframe using the *to_pandas* function.\n",
    "\n",
    "While a Pandas Spark Dataframe is distributed and partitioned across multiple machines/cores, the Pandas Dataframe is in a single machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65fffa02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65fffa02",
    "outputId": "146816ae-d748-4574-b289-99103a046ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Name  Age  Educational level Company\n",
      "0     Andrew   55                1.0    Good\n",
      "1   Bernhard   43                2.0    Good\n",
      "2   Carolina   37                5.0     Bad\n",
      "3     Dennis   82                3.0    Good\n",
      "4        Eve   23                3.2     Bad\n",
      "5       Fred   46                5.0    Good\n",
      "6    Gwyneth   38                4.2     Bad\n",
      "7     Hayden   50                4.0     Bad\n",
      "8      Irene   29                4.5     Bad\n",
      "9      James   42                4.1    Good\n",
      "10     Kevin   35                4.5     Bad\n",
      "11       Lea   38                2.5    Good\n",
      "12    Marcus   31                4.8     Bad\n",
      "13     Nigel   71                2.3    Good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "dfPDF = df.to_pandas()\n",
    "\n",
    "print(dfPDF)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
