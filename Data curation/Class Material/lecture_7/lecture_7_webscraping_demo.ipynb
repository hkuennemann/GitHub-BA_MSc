{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OuPr64VtFJUB"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"wuTsolURFJUE"},"source":["## The requests library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86wr8jrjFJUG"},"outputs":[],"source":["import requests\n","page = requests.get(\"https://en.wikipedia.org/wiki/Nova_School_of_Business_and_Economics\")\n","page"]},{"cell_type":"markdown","metadata":{"id":"845on138FJUI"},"source":["After running our request, we get a Response object. This object has a ```status_code``` property, which indicates if the page was downloaded successfully:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7hKJM2uFJUI"},"outputs":[],"source":["page.status_code"]},{"cell_type":"markdown","source":["A code \"200\" means that the request was successfully received, understood, and accepted."],"metadata":{"id":"_Vu8Mpk8F6dL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"45IDHH2dFJUJ"},"outputs":[],"source":["requests.get(\"https://en.wikipedia.org/wiki/Nova_School_of_Business_and_Economics2\")"]},{"cell_type":"markdown","metadata":{"id":"0bcGTTNZFJUJ"},"source":["As you can see, you may be request the wrong page and return different status code. Check the full list of HTTP Status Code at [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)"]},{"cell_type":"markdown","metadata":{"id":"lmteAA8ZFJUJ"},"source":["We can print out the HTML content of the page using the ```content``` property:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5moCk_CFJUK"},"outputs":[],"source":["page.content"]},{"cell_type":"markdown","metadata":{"id":"LOezDBOBFJUK"},"source":["## Parsing a page with BeautifulSoup\n","\n","As you can see above, we now have downloaded an HTML document.\n","\n","We can use the BeautifulSoup library to parse this document, and extract the text from the ```p``` tag. We first have to import the library, and create an instance of the BeautifulSoup class to parse our document:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8N8d-nSJFJUL"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","soup = BeautifulSoup(page.content, 'html.parser')"]},{"cell_type":"markdown","metadata":{"id":"_eOc1JoCFJUL"},"source":["We can now print out the HTML content of the page, formatted nicely, using the prettify method on the BeautifulSoup object:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4-lLgshFJUL"},"outputs":[],"source":["print(soup.prettify())"]},{"cell_type":"markdown","metadata":{"id":"3eF0CAepFJUM"},"source":["As all the tags are nested, we can move through the structure one level at a time. We can first select all the elements at the top level of the page using the children property of soup. Note that children returns a list generator, so we need to call the list function on it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTn-85Q2FJUP"},"outputs":[],"source":["list(soup.children)"]},{"cell_type":"markdown","metadata":{"id":"IAGtUwPaFJUQ"},"source":["The above tells us that there are two tags at the top level of the page — the initial <!DOCTYPE html> tag, and the <html> tag. There is a newline character (n) in the list as well. Let’s see what the type of each element in the list is:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBGZlOBCFJUQ"},"outputs":[],"source":["[type(item) for item in list(soup.children)]"]},{"cell_type":"markdown","metadata":{"id":"s74RZBJqFJUR"},"source":["As you can see, all of the items are BeautifulSoup objects. The first is a ```Doctype``` object, which contains information about the type of the document. The second is a ```NavigableString```, which represents text found in the HTML document. The final item is a ```Tag``` object, which contains other nested tags. The most important object type, and the one we’ll deal with most often, is the Tag object."]},{"cell_type":"markdown","metadata":{"id":"L676ffWoFJUR"},"source":["The Tag object allows us to navigate through an HTML document, and extract other tags and text. You can learn more about the various BeautifulSoup objects [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#kinds-of-objects)."]},{"cell_type":"markdown","metadata":{"id":"F3j2kiyIFJUR"},"source":["We can now select the html tag and its children by taking the third item in the list:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-RE5eOZFJUR"},"outputs":[],"source":["html = list(soup.children)[2]\n","html"]},{"cell_type":"markdown","metadata":{"id":"MEqDvCPmFJUS"},"source":["Each item in the list returned by the children property is also a BeautifulSoup object, so we can also call the children method on html.\n","\n","Now, we can find the children inside the html tag:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aSTJ48rfFJUS"},"outputs":[],"source":["list(html.children)"]},{"cell_type":"markdown","metadata":{"id":"UUqaIjrkFJUS"},"source":["As you can see above, there are many tags here, head, title, script and body. We want to extract the text inside the title tag, so we’ll dive into the head first:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1ls1DMOFJUT"},"outputs":[],"source":["head = list(html.children)[1]\n","head"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"801QjYf6FJUT"},"outputs":[],"source":["title = list(head.children)[3]\n","title"]},{"cell_type":"markdown","metadata":{"id":"eVDA9VuMFJUT"},"source":["Once we’ve isolated the tag, we can use the get_text method to extract all of the text inside the tag:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDy6aJIEFJUT"},"outputs":[],"source":["title.get_text()"]},{"cell_type":"markdown","metadata":{"id":"chIwdakMFJUU"},"source":["### Finding all instances of a tag at once\n","\n","What we did above was useful for figuring out how to navigate a page, but it took a lot of commands to do something fairly simple. If we want to extract a single tag, we can instead use the ```find``` or ```find_all``` method, which will find all the instances of a tag on a page."]},{"cell_type":"markdown","metadata":{"id":"BBc3jWtdFJUU"},"source":["If you only want to find the first instance of a tag, you can use the ```find``` method, which will return a single BeautifulSoup object:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAwJ4YZQFJUU"},"outputs":[],"source":["soup.find('title').get_text()"]},{"cell_type":"markdown","metadata":{"id":"HecVQcIqFJUU"},"source":["Note that find_all returns a list, so we’ll have to loop through, or use list indexing, it to extract text:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"395tMuInFJUV"},"outputs":[],"source":["soup.find_all('p')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdzTLy4OFJUV"},"outputs":[],"source":["[paragraph for paragraph in list(soup.find_all('p'))\n"," if \"Financial Times\" in paragraph.get_text()]"]},{"cell_type":"markdown","metadata":{"id":"5WqCIAgbFJUV"},"source":["### Searching for tags by class and id\n","We introduced classes and ids earlier, but it probably wasn’t clear why they were useful. Classes and ids are used by CSS to determine which HTML elements to apply certain styles to. We can also use them when scraping to specify specific elements we want to scrape."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVPlmEdaFJUV"},"outputs":[],"source":["soup.find_all(class_=\"street-address\")"]},{"cell_type":"code","source":[],"metadata":{"id":"201ZWs0KInNi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5I0hkxr1FJUW"},"outputs":[],"source":["soup.find_all(class_=\"nickname\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Rkwxe9sFJUW"},"outputs":[],"source":["soup.find_all(\"h2\")"]},{"cell_type":"markdown","metadata":{"id":"te_CBw0pFJUW"},"source":["Similarly, you can also find the specific tag with its id:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZRJtWz-FJUY"},"outputs":[],"source":["soup.find_all(\"h2\", id=\"mw-toc-heading\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4GgTpikFJUZ"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","from lxml import html\n","soup = BeautifulSoup(page.content, 'lxml')\n","tree = html.fromstring(page.content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAqbRhoiFJUZ"},"outputs":[],"source":["tree.xpath('//*[@id=\"mw-content-text\"]/div[1]/table')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQJgUDwWFJUZ"},"outputs":[],"source":["for row in tree.xpath('//*[@id=\"mw-content-text\"]/div[1]/table//tr'):\n","    print(row.xpath('//td//text()'))"]},{"cell_type":"markdown","metadata":{"id":"IjhZa6_6FJUa"},"source":["## Data Access using API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grTtCszAFJUa"},"outputs":[],"source":["response = requests.get('https://jsonplaceholder.typicode.com/todos')"]},{"cell_type":"markdown","metadata":{"id":"_FgL2VcuFJUa"},"source":["Using ```read_json``` method to read the JSON data contained in the response:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMOAcYrTFJUb"},"outputs":[],"source":["todos = pd.read_json(response.text)\n","todos"]},{"cell_type":"markdown","metadata":{"id":"L5kI_KMEFJUb"},"source":["Write the DataFrame to the Excel file:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kk2jR9XUFJUb"},"outputs":[],"source":["todos.to_excel('todos.xls')"]},{"cell_type":"markdown","metadata":{"id":"x7d5WAqKFJUb"},"source":["Now try yourself with https://jsonplaceholder.typicode.com/photos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kqy3bTuhFJUc"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"colab":{"provenance":[{"file_id":"1gjXndA6b2Wr-LEiEeEIsxepvtgnodN-F","timestamp":1695510458512}]}},"nbformat":4,"nbformat_minor":0}